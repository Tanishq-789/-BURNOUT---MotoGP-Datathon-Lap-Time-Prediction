# 🏍️ BURNOUT - MotoGP Datathon: Lap Time Prediction

A data science solution for the IEEE-CS MUJ "BURNOUT" Kaggle competition focused on predicting average lap times of MotoGP riders using machine learning.

## 📊 Competition Overview

**Objective**: Predict `Lap_Time_Seconds` — the average lap time per rider per race — based on rich MotoGP data including rider stats, track features, weather, tire wear, bike specifications, and historical performance.

## 🗂️ Dataset Details

The dataset spans multiple MotoGP seasons with features including:

- **Race Info**: `Circuit_Length_km`, `Laps`, `Grid_Position`, `Track_Condition`, `Weather`
- **Performance**: `Avg_Speed_kmh`, `Championship_Points`, `Position`, `Points`
- **Technical**: `Tire_Compound_Front/Rear`, `Tire_Degradation_Factor_per_Lap`, `Pit_Stop_Duration_Seconds`
- **Environmental**: `Ambient_Temperature_Celsius`, `Track_Temperature_Celsius`, `Humidity_%`
- **Career Stats**: `Starts`, `Finishes`, `Wins`, `Podiums`, `Years_active`

**Target**: `Lap_Time_Seconds`

## 🛠️ Approach

### 1. Data Preprocessing
- Removed high-cardinality/redundant columns (e.g., rider/bike names).
- Handled missing values and categorized relevant columns.
- Applied **target encoding** to `rider`, `team`, and `bike`.
- Engineered new features like:
  - `temp_diff`: Difference between track and air temperature
  - `speed_per_km`: Average speed normalized by circuit length
  - `degradation_per_corner`: Tire wear normalized by corners per lap
- Removed outliers using quantile filtering.
- Transformed target variable using √ for stability.

### 2. Modeling
Trained three base models using 5-fold cross-validation:
- **XGBoost** (`gpu_hist`)
- **LightGBM**
- **CatBoost** (with GPU)

### 3. Stacking
- Combined base model outputs using a **Ridge Regression** stacker (alpha tuned via Optuna).
- Achieved improved RMSE over individual models.

### 4. Inference
- Applied the same preprocessing to test data.
- Made predictions with base models and stacked them for final output.
- Reversed the √ transform to produce final `Lap_Time_Seconds`.

## 📈 Evaluation

Used Root Mean Squared Error (RMSE) as the evaluation metric for both cross-validation folds and stacked predictions.

## 📦 Files

- `train.csv`, `test.csv`, `val.csv`: Data files
- `sample_submission.csv`: Format for final output
- `solution.csv`: Final predictions file generated by the pipeline

## 🧠 Libraries Used

- `XGBoost`, `LightGBM`, `CatBoost`
- `scikit-learn` (KFold, Ridge, MSE)
- `Pandas`, `NumPy`
- `joblib` for model saving

## ✅ Final Output

- Final predictions saved to `solution.csv` for submission.
- All models (base + stacked) are persisted using joblib for reuse.

---

## 🔗 Useful Links & Connect 🤝

- 📂 **Dataset**: [Kaggle Competition Page](https://www.kaggle.com/competitions/burnout-datathon-ieeecsmuj/data)
- 📘 **Notebook Code**: [My Kaggle Notebook](https://www.kaggle.com/code/tanishqshinde775/data-dashers)
- 🙋‍♂️ **Follow me**: [Tanishq Shinde on Kaggle](https://www.kaggle.com/tanishqshinde775)

💬 Let's connect, collaborate, and build cool things together 🚀
